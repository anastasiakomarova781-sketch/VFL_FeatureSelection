================================================================================
                    PSO: Метод отбора признаков
                    Отбор признаков на основе Information Gain
                    Работает с признаками активной и пассивной сторон
================================================================================

БЫСТРЫЙ СТАРТ - КОМАНДЫ ЗАПУСКА
================================================================================

1. УСТАНОВКА ЗАВИСИМОСТЕЙ
--------------------------
pip install -r ../requirements.txt

Или вручную:
pip install numpy pandas scikit-learn

2. ЗАПУСК МЕТОДА (из корня проекта)
------------------------------------
python3 PSO/pso_run.py

Или из папки PSO:
cd PSO
python3 pso_run.py

3. НАСТРОЙКА ПАРАМЕТРОВ
-----------------------
Откройте файл PSO/pso_run.py и измените параметры в методе main():

fs = FedSDGFSPlain(
    max_depth=1,     # Максимальная глубина дерева (1 = decision stump)
    min_gain=1e-5    # Минимальный порог gain для отбора признака
)

ОПИСАНИЕ МЕТОДА
================================================================================

PSO (Plaintext Selection Optimization) - это метод отбора признаков для 
вертикального федеративного обучения, работающий без шифрования.

ОСОБЕННОСТИ:
- Работает без шифрования (быстрое выполнение)
- Работает с признаками активной и пассивной сторон (объединяет датасеты)
- Использует Information Gain для оценки важности признаков
- Основан на Decision Tree Classifier (decision stump)
- Отбирает признаки из обеих сторон одновременно
- Embedded-отбор признаков (отбор встроен в процесс обучения)

ОСНОВНЫЕ КОМПОНЕНТЫ
================================================================================

1. Файл PSO_functions.py:
   Содержит класс FedSDGFSPlain с методами:
   - fit(X, y) - обучение и отбор признаков
   - transform(X) - применение отбора к новым данным
   - fit_transform(X, y) - обучение и трансформация за один вызов
   - _compute_feature_gain(X_feat, y) - вычисление Information Gain для признака

2. Файл pso_run.py:
   Основной скрипт запуска метода:
   - Загружает данные из Data/active_dataset_test.csv (активная сторона)
   - Загружает данные из Data/passive_dataset_test.csv (пассивная сторона)
   - Объединяет датасеты по id
   - Создает объект для отбора признаков
   - Выполняет отбор признаков из обеих сторон
   - Сохраняет результаты в Data/active_dataset_selected.csv
   - Выводит информацию о gain каждого признака (активные и пассивные)

3. Метод отбора признаков:
   - Для каждого признака создается Decision Tree глубины 1 (decision stump)
   - Вычисляется Information Gain на основе Gini impurity
   - Признаки с gain выше порога отбираются

Импорт класса:
   from PSO_functions import FedSDGFSPlain
   
Примечание: Класс FedSDGFSPlain реализует метод PSO для отбора признаков.

ЭТАПЫ РАБОТЫ МЕТОДА (на основе кода)
================================================================================

ЭТАП 1: Инициализация объекта PSO
-------------------------------------------
- Устанавливается максимальная глубина дерева (max_depth)
- Устанавливается минимальный порог gain (min_gain)
- Инициализируются пустые списки для хранения результатов

Параметры:
- max_depth=1: создается decision stump (дерево глубины 1)
- min_gain=1e-5: минимальный вклад признака для отбора

ЭТАП 2: Загрузка и подготовка данных
-------------------------------------
- Загружается CSV файл с данными активной стороны (с таргетом)
- Загружается CSV файл с данными пассивной стороны (без таргета)
- Датасеты объединяются по столбцу 'id' (inner join)
- Извлекаются признаки X из обеих сторон (без столбцов 'id' и 'target')
- Извлекается целевая переменная y (столбец 'target' из активной стороны)

Код:
    df_active = pd.read_csv("Data/active_dataset_test.csv")
    df_passive = pd.read_csv("Data/passive_dataset_test.csv")
    df_merged = pd.merge(df_active, df_passive, on="id", how="inner")
    X = df_merged.drop(columns=["id", "target"])
    y = df_merged["target"]

ЭТАП 3: Обучение модели отбора признаков (метод fit)
-----------------------------------------------------
Для каждого признака выполняется:

3.1. Извлечение одного признака
    - Берется один признак из матрицы X
    - Это имитирует SecureBoost, где каждый признак оценивается независимо

3.2. Создание Decision Tree для признака
    - Создается DecisionTreeClassifier с max_depth=1 (decision stump)
    - Критерий разделения: "gini" (Gini impurity)
    - Дерево обучается ТОЛЬКО на одном признаке

3.3. Вычисление Information Gain
    - Вычисляется impurity в корне дерева (до сплита)
    - Если дерево не смогло сделать сплит (node_count == 1), gain = 0.0
    - Иначе вычисляется взвешенная impurity после сплита
    - Information Gain = impurity_parent - impurity_after_split

3.4. Сохранение gain признака
    - Gain сохраняется в словаре gains[feature] = gain

Код:
    for feature in X.columns:
        gain = self._compute_feature_gain(X[[feature]], y)
        gains[feature] = gain

ЭТАП 4: Отбор признаков на основе порога
-----------------------------------------
- Все признаки с gain > min_gain отбираются
- Список отобранных признаков сохраняется в selected_features_

Код:
    self.selected_features_ = [
        f for f, g in gains.items() if g > self.min_gain
    ]

ЭТАП 5: Применение отбора к данным (метод transform)
-----------------------------------------------------
- Из исходных данных выбираются только отобранные признаки
- Возвращается DataFrame с отобранными признаками

Код:
    return X[self.selected_features_]

ЭТАП 6: Сохранение результатов
------------------------------
- Создается итоговый DataFrame: id + отобранные признаки + target
- Результат сохраняется в CSV файл

Код:
    result = pd.concat([df[["id"]], X_selected, y], axis=1)
    output_path = os.path.join(project_root, "Data", "active_dataset_selected.csv")
    result.to_csv(output_path, index=False)

ЭТАП 7: Вывод информации о результатах
---------------------------------------
- Выводится список всех признаков и их gain значения
- Позволяет увидеть вклад каждого признака

Код:
    for f, g in fs.feature_gains_.items():
        print(f"{f}: gain={g:.6f}")

ОСНОВНЫЕ МЕТОДЫ КЛАССА
================================================================================

1. __init__(max_depth=3, min_gain=1e-6)
   Инициализация объекта отбора признаков
   - max_depth: максимальная глубина дерева (обычно 1 для decision stump)
   - min_gain: минимальный порог gain для отбора признака

2. fit(X, y)
   Обучение модели отбора признаков
   - X: DataFrame с признаками
   - y: Series с целевой переменной
   - Возвращает self для chaining

3. transform(X)
   Применение отбора признаков к данным
   - X: DataFrame с признаками
   - Возвращает DataFrame только с отобранными признаками

4. fit_transform(X, y)
   Обучение и трансформация за один вызов
   - X: DataFrame с признаками
   - y: Series с целевой переменной
   - Возвращает DataFrame с отобранными признаками

5. _compute_feature_gain(X_feat, y)
   Вычисление Information Gain для одного признака
   - X_feat: DataFrame с одним признаком
   - y: Series с целевой переменной
   - Возвращает значение gain (float)

ПРИНЦИП РАБОТЫ Information Gain
================================================================================

Information Gain показывает, насколько хорошо признак разделяет классы:

1. Вычисляется Gini impurity в корне (до разделения):
   impurity_parent = Gini(y)

2. Создается decision stump (дерево глубины 1) для признака
   Дерево находит оптимальный порог разделения

3. Вычисляется взвешенная impurity после разделения:
   impurity = (n_left/n) * Gini(y_left) + (n_right/n) * Gini(y_right)

4. Information Gain:
   gain = impurity_parent - impurity

Чем выше gain, тем лучше признак разделяет классы и тем важнее он для модели.

ПРЕИМУЩЕСТВА МЕТОДА
================================================================================

1. Скорость:
   - Работает без шифрования (быстро)
   - Подходит для быстрого тестирования и прототипирования

2. Простота:
   - Не требует шифрования и безопасного взаимодействия между сторонами
   - Работает с объединенными данными активной и пассивной сторон
   - Все вычисления выполняются локально

3. Интерпретируемость:
   - Понятная метрика (Information Gain)
   - Можно увидеть вклад каждого признака

4. Embedded отбор:
   - Отбор признаков встроен в процесс обучения
   - Учитывает взаимодействие признака с таргетом

СТРУКТУРА ПРОЕКТА
================================================================================

VFL_FeatureSelection/
├── PSO/                              # Метод PSO (отбор признаков)
│   ├── PSO_functions.py             # Класс FedSDGFSPlain (реализация PSO)
│   ├── pso_run.py                   # Скрипт запуска PSO
│   └── README_PSO.txt               # Этот файл
├── fedsdg/                           # Метод FedSDG-FS (с шифрованием)
│   ├── fedsdg_fs_article.py
│   ├── run_fedsdg_fs.py
│   └── README_FedSDG.txt
├── Data/
│   ├── active_dataset_test.csv      # Входные данные (активная сторона)
│   ├── passive_dataset_test.csv      # Входные данные (пассивная сторона)
│   └── active_dataset_selected.csv  # Результат отбора
└── requirements.txt

ФОРМАТ ВХОДНЫХ ДАННЫХ
================================================================================

Активный датасет (active_dataset_test.csv):
- Обязательные столбцы: 'id', 'target'
- Столбцы с признаками: feat_a_* (активные признаки)

Пассивный датасет (passive_dataset_test.csv):
- Обязательные столбцы: 'id'
- Столбцы с признаками: feat_b_* (пассивные признаки)
- НЕТ столбца 'target'

Пример структуры активного датасета:
id,feat_a_00,feat_a_01,feat_a_02,...,target
82446,0.7269885,72.7,1052846.6,...,0
133988,0.0,0.0,2.0,...,1

Пример структуры пассивного датасета:
id,feat_b_00,feat_b_01,feat_b_02,...
82446,12,0.0,,...
133988,13,0.0,0.0,...

Важно: Оба датасета должны иметь общие значения в столбце 'id' для объединения.

ФОРМАТ ВЫХОДНЫХ ДАННЫХ
================================================================================

active_dataset_selected.csv:
- Столбцы: 'id', отобранные признаки, 'target'
- Содержит только признаки с gain > min_gain

ПРИМЕР ИСПОЛЬЗОВАНИЯ
================================================================================

1. Подготовьте данные в формате CSV:
   - Активный датасет: Data/active_dataset_test.csv (с 'id' и 'target')
   - Пассивный датасет: Data/passive_dataset_test.csv (с 'id', без 'target')

2. Убедитесь, что оба датасета имеют общие значения в столбце 'id'

3. Запустите скрипт:
   python3 PSO/pso_run.py

4. Результаты будут сохранены в:
   - Data/active_dataset_selected.csv (с отобранными признаками обеих сторон)

5. В консоли будет выведена информация о:
   - Количестве загруженных образцов
   - Количестве активных и пассивных признаков
   - Gain значении каждого признака (с пометкой АКТИВНЫЙ/ПАССИВНЫЙ)
   - Отобранных признаках из обеих сторон

ПРОИЗВОДИТЕЛЬНОСТЬ
================================================================================

Метод работает очень быстро (без шифрования):
- 1000 образцов: ~1-2 секунды
- 10000 образцов: ~5-10 секунд
- 60000 образцов: ~30-60 секунд

РЕКОМЕНДАЦИИ ПО ПАРАМЕТРАМ
================================================================================

max_depth:
- 1: рекомендуется для быстрого отбора (decision stump)
- 2-3: для более сложных взаимодействий признаков

min_gain:
- 1e-5: низкий порог (отберет больше признаков)
- 1e-3: средний порог
- 1e-2: высокий порог (отберет только очень важные признаки)

УСТРАНЕНИЕ ПРОБЛЕМ
================================================================================

Проблема: "ModuleNotFoundError: No module named 'pandas'"
Решение: Установите зависимости: pip install numpy pandas scikit-learn

Проблема: "ModuleNotFoundError: No module named 'PSO_functions'"
Решение: Убедитесь, что запускаете скрипт из корня проекта или из папки PSO

Проблема: "FileNotFoundError: Data/active_dataset_test.csv"
Решение: Убедитесь, что файл находится в правильной папке Data/

Проблема: Не отобраны признаки
Решение: Уменьшите min_gain (например, 1e-6) для более мягкого отбора

Проблема: Отобрано слишком много признаков
Решение: Увеличьте min_gain (например, 1e-3) для более строгого отбора

ОТЛИЧИЯ ОТ МЕТОДА FEDSDG-FS
================================================================================

Метод PSO (папка PSO):
- Нет шифрования Paillier
- Работает с объединенными данными активной и пассивной сторон
- Быстрое выполнение (без шифрования)
- Использует Information Gain для отбора признаков
- Подходит для быстрого тестирования и прототипирования

Метод FedSDG-FS (папка fedsdg):
- Использует шифрование Paillier
- Безопасное взаимодействие между сторонами
- Работает с активной и пассивной сторонами раздельно
- Медленное выполнение (из-за шифрования)
- Подходит для продакшена с требованиями безопасности

================================================================================
                            Конец документации
================================================================================

